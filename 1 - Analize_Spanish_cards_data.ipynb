{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc60fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540205d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = pd.read_csv(\"csv/cards.csv\")\n",
    "translate = pd.read_csv(\"csv/foreign_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Portuguese (Brazil), Korean, Japanese, French, German, Chinese Traditional, Chinese Simplified\n",
    "esp = translate[translate['language']==\"Spanish\"] \n",
    "esp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995cd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(esp)/len(cards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca5367",
   "metadata": {},
   "source": [
    "### Atención:\n",
    "\n",
    "* en español, hay 33969 cartas, que corresponde a un 52.67% del universo de cartas oficiales en inglés.\n",
    "\n",
    "### Objetivo 1: \n",
    "\n",
    "* reemplazar entradas 'nan' del campo *type* por su texto en inglés.\n",
    "> ESTRATEGIA: emplear el campo 'uuid' y corelacionarlo con su versión original en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstToken(string, char):\n",
    "    t = string.index(char,0)\n",
    "    return string[0:t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchByToken(df, token):\n",
    "    i = 0\n",
    "    R = []\n",
    "    for i in range(len(df)):\n",
    "        if (df[i] == token):\n",
    "            R.append(i)\n",
    "            return R \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = list(cards['uuid'])\n",
    "uuid_first_token = [getFirstToken(x, '-') for x in uuid]\n",
    "print(uuid[0])\n",
    "print(uuid_first_token[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18b056",
   "metadata": {},
   "source": [
    "> Una opción para encontrar mas rápido es construir un bag of words y vectorizar el resultado, sin embargo, generar lo anterior conlleva demasiada memoria (+80Gb). Se opta por trabajar consultas por términos. El parámetro que es global y es independiente del idioma es el **uuid**. Para reducir el tiempo de busqueda, solo se usará el primer token del uuid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicidad_uuid():\n",
    "    start = time.time()\n",
    "    not_unique= []\n",
    "    for i in range( len(uuid_first_token)):\n",
    "        a = getFirstToken(uuid[i],'-')\n",
    "        st = searchByToken(uuid_first_token, a)\n",
    "        if(len(st)>1):\n",
    "            not_unique.append(i)\n",
    "    if (len(not_unique)>0):\n",
    "        print('NO HAY UNICIDAD, para un primer token de uuid hay más de un resultado')\n",
    "    else:\n",
    "        print('UNICIDAD, se cumple unicidad de tokens')\n",
    "        \n",
    "    end = time.time()\n",
    "    print ('time:', end - start)\n",
    "    return not_unique\n",
    "\n",
    "#unico = unicidad_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc522e76",
   "metadata": {},
   "source": [
    "Se probó si el primer token del uuid es único. Los resultados lo comprueban, por lo tanto, buscamos solamente con la lista de **uuid_first_token** (primer token de cada uuid del universo total de cartas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e371548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uuid_from_nan_values(LIST):\n",
    "    type_nan_uuid = []\n",
    "    type_nan_id = []\n",
    "    for i in range(len(LIST)):\n",
    "        if (type(LIST[i]) == float):\n",
    "            type_nan_id.append(i)\n",
    "            _token = getFirstToken(esp.iloc[i].uuid,'-')\n",
    "            type_nan_uuid.append(_token)\n",
    "    \n",
    "    return type_nan_id, type_nan_uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_TYPES = list(esp['type'])\n",
    "types_id, types_uuid = get_uuid_from_nan_values(LIST_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac939511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CARDS with 'type' = NaN:\",len(types_uuid))\n",
    "_esp = esp.iloc[types_id[0]]\n",
    "print(_esp)\n",
    "print(_esp['name'], _esp['id'], _esp['uuid'])\n",
    "\n",
    "_esp_id = _esp['index']\n",
    "_card = cards.iloc[_esp_id]\n",
    "print(_card['name'], _card['id'], _card['uuid'],' <- NOT MATCH with id/index values!')\n",
    "\n",
    "r = searchByToken(uuid_first_token, getFirstToken(_esp['uuid'],'-'))[0]\n",
    "print('\\n** Real id card in english:', r)\n",
    "\n",
    "_card = cards.iloc[r]\n",
    "print(_card['name'], _card['id'], _card['uuid'],' <- MATCH by uuid!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c6dbc",
   "metadata": {},
   "source": [
    "> Hay 7214 cartas en español sin un type definido.\n",
    "\n",
    "Analizando la primera carta en español con type = NaN, probamos si con el id o el index dado en el dataframe podemos llegar a la carta en inglés, sin embargo, los uuid no coinciden.\n",
    "\n",
    "Haciendo match via uuid es la opción.\n",
    "\n",
    "Linkear types nan a inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_types_Spanish(attribute, list_ids, uuids): \n",
    "    start = time.time()\n",
    "    for i in range(len(list_ids)):\n",
    "        index_esp = list_ids[i]\n",
    "        first_token = uuids[i]\n",
    "        index_eng = searchByToken(uuid_first_token, first_token)[0]\n",
    "        _card = cards.iloc[index_eng]\n",
    "        esp[attribute].iloc[index_esp] = _card[attribute]\n",
    "    end = time.time()\n",
    "    print ('time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_types_Spanish('type', types_id, types_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bbb8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_esp = esp.iloc[types_id[0]]\n",
    "print(_esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "esp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633173ec",
   "metadata": {},
   "source": [
    "### ya tenemos linkeados los types en español."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c63cf96",
   "metadata": {},
   "source": [
    "## Objetivo 2:\n",
    "Ahora completaremos los valores de text y flavorText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute in ['text', 'flavorText']:\n",
    "    print('MATCHING ', attribute)\n",
    "    LIST_ATTR = list(esp[attribute])\n",
    "    ids, uuids = get_uuid_from_nan_values(LIST_ATTR)\n",
    "    match_types_Spanish(attribute, ids, uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55123432",
   "metadata": {},
   "outputs": [],
   "source": [
    "esp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ffbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullableTest(attribute):\n",
    "    e = list(esp[attribute])[0:100]\n",
    "    for i in range(len(e)):\n",
    "        if (type(e[i]) == float):\n",
    "            _test = esp.iloc[i]\n",
    "            r = searchByToken(uuid_first_token, getFirstToken(_test['uuid'],'-'))[0]\n",
    "            _card = cards.iloc[r]\n",
    "            print(i, e[i], _test['uuid'],' english->' ,_card['id'], _card[attribute], _card['uuid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208214e",
   "metadata": {},
   "source": [
    "Se comprueba si el algoritmo funcionó, probando por cada atributo nulo en el dataframe si también lo es en el dataframe inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1714d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for attribute in ['text', 'flavorText']:\n",
    "    print('Testing: ', attribute)\n",
    "    print('\\nid_lang, attribute, uuid_lang, || english_id, english_atribute, english_uuid')\n",
    "    nullableTest(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ee4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "esp.to_csv('./output/spanish_cards.csv',index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fcddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
